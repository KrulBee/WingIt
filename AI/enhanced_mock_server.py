#!/usr/bin/env python3
"""
Enhanced Vietnamese Profanity Detection Server (No PyTorch Required)
Uses advanced pattern matching and Vietnamese language rules
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import re
import time
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

class EnhancedVietnameseProfanityDetector:
    """Enhanced Vietnamese profanity detector without ML dependencies"""
    
    def __init__(self):
        self.model_loaded = True
        
        # Comprehensive Vietnamese profanity patterns
        self.profanity_patterns = {
            # Direct profanity
            'explicit': [
                r'\b(ƒëm|dm|ƒëmm|ƒëcm|dcm)\b',
                r'\b(vcl|vkl|vl)\b', 
                r'\b(cc|cl|lol|loz)\b',
                r'\b(shit|fuck|bitch)\b',
                r'\b(ƒëƒ©|di|ƒë·ªâ)\b',
                r'\b(c·∫∑c|bu·ªìi|l·ªìn)\b',
                r'\b(ch·∫øt ti·ªát|ch·∫øt t√≠a)\b',
            ],
            
            # Contextual profanity
            'contextual': [
                r'\b(th·∫±ng|con|ƒë·ªì)\s+(ngu|kh√πng|ƒëi√™n|ƒë·∫ßn|ng·ªëc)\b',
                r'\b(ƒë·ªì|con)\s+(ƒëƒ©|kh·ªën|kh·ªâ|ch√≥|heo)\b',
                r'\b(m√†y|mi|m)\s+(ngu|kh√πng|ƒëi√™n)\b',
                r'\b(tao|t)\s+(gh√©t|ch·ª≠i|ƒë√°nh)\s+(m√†y|mi|m)\b',
                r'\b(c√∫t|bi·∫øn|ƒëi ch·∫øt)\s+(ƒëi|m·∫π|m√†y)\b',
            ],
            
            # Disguised profanity (with special characters)
            'disguised': [
                r'\b(d[*@#]m|ƒë[*@#]m)\b',
                r'\b(v[*@#]l|vc[*@#])\b',
                r'\b(sh[*@#]t|f[*@#]ck)\b',
                r'\b(ƒë[*@#]|d[*@#])\b',
            ],
            
            # Hate speech patterns
            'hate_speech': [
                r'\b(gi·∫øt|ƒë√°nh|ch√©m|ƒë·∫≠p)\s+(ch·∫øt|t∆°i t·∫£|tan n√°t)\b',
                r'\b(ƒëi ch·∫øt|ch·∫øt ƒëi|ch·∫øt m·∫π)\b',
                r'\b(kh·ªßng b·ªë|ph√° ho·∫°i|t√†n ph√°)\b',
            ]
        }
        
        # Confidence weights for different pattern types
        self.confidence_weights = {
            'explicit': 0.95,
            'contextual': 0.85,
            'disguised': 0.80,
            'hate_speech': 0.90
        }
        
        # Compile patterns for better performance
        self.compiled_patterns = {}
        for category, patterns in self.profanity_patterns.items():
            self.compiled_patterns[category] = [
                re.compile(pattern, re.IGNORECASE | re.UNICODE) 
                for pattern in patterns
            ]
        
        logger.info("Enhanced Vietnamese Profanity Detector initialized")
        logger.info(f"Loaded {sum(len(p) for p in self.profanity_patterns.values())} profanity patterns")
    
    def is_ready(self):
        return self.model_loaded
    
    def preprocess_text(self, text):
        """Preprocess Vietnamese text for better detection"""
        if not text:
            return ""
        
        # Normalize Vietnamese text
        text = text.strip()
        
        # Handle common character substitutions
        substitutions = {
            '0': 'o', '1': 'i', '3': 'e', '4': 'a', '5': 's',
            '@': 'a', '#': '', '*': '', '+': 't'
        }
        
        normalized = text
        for old, new in substitutions.items():
            normalized = normalized.replace(old, new)
        
        return normalized
    
    def detect_profanity(self, text):
        """Enhanced profanity detection with pattern matching"""
        if not text or not isinstance(text, str):
            return {
                'is_profane': False,
                'confidence': 0.0,
                'toxic_spans': [],
                'processed_text': text or '',
                'detector_type': 'enhanced_pattern_matching'
            }
        
        # Preprocess text
        processed_text = self.preprocess_text(text)
        original_text = text.strip()
        
        # Find matches
        toxic_spans = []
        max_confidence = 0.0
        detected_categories = []
        
        for category, patterns in self.compiled_patterns.items():
            for pattern in patterns:
                matches = list(pattern.finditer(processed_text.lower()))
                
                for match in matches:
                    confidence = self.confidence_weights[category]
                    max_confidence = max(max_confidence, confidence)
                    
                    # Map back to original text positions (approximate)
                    start, end = match.span()
                    toxic_spans.append([start, end])
                    detected_categories.append(category)
        
        # Remove duplicate spans
        toxic_spans = self._merge_overlapping_spans(toxic_spans)
        
        # Determine if text is profane
        is_profane = len(toxic_spans) > 0 and max_confidence > 0.7
        
        # Add context analysis
        if not is_profane:
            context_score = self._analyze_context(processed_text)
            if context_score > 0.6:
                is_profane = True
                max_confidence = max(max_confidence, context_score)
        
        return {
            'is_profane': is_profane,
            'confidence': float(max_confidence),
            'toxic_spans': toxic_spans,
            'processed_text': original_text,
            'detected_categories': list(set(detected_categories)),
            'detector_type': 'enhanced_pattern_matching',
            'analysis': {
                'pattern_matches': len(toxic_spans),
                'categories': detected_categories,
                'context_analyzed': True
            }
        }
    
    def _merge_overlapping_spans(self, spans):
        """Merge overlapping toxic spans"""
        if not spans:
            return []
        
        # Sort by start position
        spans.sort(key=lambda x: x[0])
        merged = [spans[0]]
        
        for current in spans[1:]:
            last = merged[-1]
            if current[0] <= last[1]:  # Overlapping
                merged[-1] = [last[0], max(last[1], current[1])]
            else:
                merged.append(current)
        
        return merged
    
    def _analyze_context(self, text):
        """Analyze context for implicit profanity"""
        text_lower = text.lower()
        
        # Aggressive tone indicators
        aggressive_indicators = [
            r'\b(tao|t)\s+(s·∫Ω|ph·∫£i|mu·ªën)\s+',
            r'\b(m√†y|mi|m)\s+(ph·∫£i|s·∫Ω|n√™n)\s+',
            r'\b(ƒë·ª´ng|kh√¥ng|ch·ªõ)\s+(c√≥|d√°m|ƒë∆∞·ª£c)\s+',
            r'[!]{2,}',  # Multiple exclamation marks
            r'[A-Z]{3,}',  # ALL CAPS words
        ]
        
        score = 0.0
        for pattern in aggressive_indicators:
            if re.search(pattern, text_lower):
                score += 0.2
        
        return min(score, 0.8)  # Cap at 0.8

# Initialize detector
detector = EnhancedVietnameseProfanityDetector()

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': detector.is_ready(),
        'model_type': 'enhanced_pattern_matching',
        'detector_version': '2.0',
        'patterns_loaded': sum(len(p) for p in detector.profanity_patterns.values()),
        'timestamp': time.time()
    })

@app.route('/detect', methods=['POST'])
def detect_profanity():
    """Main profanity detection endpoint"""
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({'error': 'Missing text field in request'}), 400
        
        text = data['text']
        
        if not isinstance(text, str):
            return jsonify({'error': 'Text must be a string'}), 400
        
        # Detect profanity
        result = detector.detect_profanity(text)
        
        # Add request metadata
        result['timestamp'] = time.time()
        result['model_version'] = 'enhanced-pattern-v2.0'
        
        logger.info(f"Profanity check: '{text[:50]}...' -> {'PROFANE' if result['is_profane'] else 'CLEAN'} (confidence: {result['confidence']:.2f})")
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Error in detect endpoint: {e}")
        return jsonify({
            'error': f'Server error: {str(e)}',
            'is_profane': False,
            'confidence': 0.0
        }), 500

@app.route('/model_info', methods=['GET'])
def model_info():
    """Get model information"""
    return jsonify({
        'model_name': 'Enhanced Vietnamese Profanity Detector',
        'model_type': 'pattern_matching_enhanced',
        'version': '2.0',
        'max_length': 1000,
        'labels': ['CLEAN', 'PROFANE'],
        'confidence_threshold': 0.7,
        'model_loaded': detector.is_ready(),
        'device': 'cpu',
        'features': [
            'Vietnamese language patterns',
            'Context analysis',
            'Character substitution handling',
            'Overlapping span detection',
            'Multiple profanity categories'
        ],
        'categories': list(detector.profanity_patterns.keys())
    })

@app.route('/test', methods=['GET'])
def test_endpoint():
    """Test endpoint with sample Vietnamese texts"""
    test_cases = [
        "Ch√†o b·∫°n, h√¥m nay th·∫ø n√†o?",  # Clean
        "ƒê√¢y l√† m·ªôt b√†i vi·∫øt r·∫•t hay!",  # Clean
        "Th·∫±ng ngu n√†y kh√¥ng bi·∫øt g√¨ c·∫£",  # Profane
        "ƒêm, t·∫°i sao l·∫°i th·∫ø n√†y?",     # Profane
        "M√†y ngu qu√° ƒëi",               # Profane
        "C·∫£nh ƒë·∫πp qu√°, t√¥i th√≠ch l·∫Øm!", # Clean
    ]
    
    results = []
    for text in test_cases:
        result = detector.detect_profanity(text)
        results.append({
            'text': text,
            'result': result
        })
    
    return jsonify({
        'test_results': results,
        'total_tests': len(test_cases),
        'summary': {
            'clean': sum(1 for r in results if not r['result']['is_profane']),
            'profane': sum(1 for r in results if r['result']['is_profane'])
        }
    })

if __name__ == '__main__':
    print("üöÄ Starting Enhanced Vietnamese Profanity Detection Server")
    print("üì± Model: Enhanced Pattern Matching (No PyTorch Required)")
    print("üîß Mode: Production Ready")
    print("‚úÖ Features: Context Analysis, Vietnamese Patterns, Character Substitution")
    print("=" * 70)
    
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
